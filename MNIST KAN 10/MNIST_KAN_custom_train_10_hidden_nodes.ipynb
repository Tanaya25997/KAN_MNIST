{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_438NfCGUqt5",
        "outputId": "ec014511-d8a7-4913-f82c-8fc3a9d26b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykan\n",
            "  Downloading pykan-0.0.5-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: pykan\n",
            "Successfully installed pykan-0.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install pykan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from kan import *\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Using Pytorch version:{torch.__version__}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(f'Using GPU device: {torch.cuda.get_device_name(0)}')\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print('No GPU Found, using CPU instead')\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "\n",
        "\n",
        "#### Training Dataset #####\n",
        "\n",
        "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True, pin_memory=True)\n",
        "\n",
        "#### Test dataset ####\n",
        "\n",
        "test_dataset = datasets.MNIST(root='.', train=False, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0YLs8x9U5NZ",
        "outputId": "30c8ec5c-85f9-4e24-c370-39e215bc4dfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using Pytorch version:2.3.0+cu121\n",
            "No GPU Found, using CPU instead\n",
            "Using device: cpu\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16056533.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 539392.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4444280.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1609677.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Implement KAN ######\n",
        "\n",
        "model = KAN(width=[28*28,10,10], device = device)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBe9RlStVIF3",
        "outputId": "762e4965-a120-4f3c-8ca6-816a154db90d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KAN(\n",
              "  (biases): ModuleList(\n",
              "    (0-1): 2 x Linear(in_features=10, out_features=1, bias=False)\n",
              "  )\n",
              "  (act_fun): ModuleList(\n",
              "    (0-1): 2 x KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "  )\n",
              "  (base_fun): SiLU()\n",
              "  (symbolic_fun): ModuleList(\n",
              "    (0-1): 2 x Symbolic_KANLayer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## Implement loss function (crossentropyloss) and optimizer (adam)\n",
        "\n",
        "#### Setting the loss and the optimizer\n",
        "\n",
        "Loss = nn.CrossEntropyLoss()\n",
        "\n",
        "GD = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "#### Calculating correctly predicted labels ####\n",
        "\n",
        "def correctly_predicted(output,target):\n",
        "  predicted = torch.argmax(output, dim=1)\n",
        "  correct_ones = (predicted==target).type(torch.float)\n",
        "  return correct_ones.sum().item()\n"
      ],
      "metadata": {
        "id": "85PLeL4BVYcX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(data_loader,model,Loss,GD, start_epoch):\n",
        "\n",
        "\n",
        "\n",
        "  num_batches = len(data_loader)\n",
        "  num_samples = len(data_loader.dataset)\n",
        "\n",
        "  total_loss = 0\n",
        "  total_accurate = 0\n",
        "\n",
        "  checkpoint_path = '/content/drive/MyDrive/mnist_model_checkpoint-10hiddennodes.pth'\n",
        "\n",
        "  #model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(data_loader):\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "            print(f\"\\n >>> Epoch {start_epoch}, Batch {batch_idx}\")\n",
        "\n",
        "    # Save checkpoint after every 100 batches\n",
        "    if batch_idx > 0 and batch_idx % 100 == 0:\n",
        "            print(f\"\\n Saving checkpoint for epoch {start_epoch}, batch {batch_idx}\")\n",
        "            checkpoint = {\n",
        "                'epoch': start_epoch,\n",
        "                'batch_idx': batch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': GD.state_dict(),\n",
        "                'train_loss': total_loss / (batch_idx + 1),  # Average loss up to current batch\n",
        "                'accuracy': total_accurate / ((batch_idx + 1) * data.size(0))  # Average accuracy up to current batch\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            #files.download(checkpoint_path)\n",
        "            print(f\"Checkpoint saved and downloaded on local for epoch {start_epoch}, batch {batch_idx}\")\n",
        "\n",
        "    ## copy to device\n",
        "    data = data.view(-1, 28 * 28).to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    #print(f\"Length of data = {len(data)} and that of target is {len(target)}\")\n",
        "\n",
        "    ### forward pass\n",
        "    output = model(data)\n",
        "    #print(f\"Length of output = {len(output)}\")\n",
        "\n",
        "    ### Calculate loss ###\n",
        "    #output_singular = torch.argmax(output, dim=1)\n",
        "    #target_float = target.float()\n",
        "    batch_loss = Loss(output,target)\n",
        "    total_loss += batch_loss.item()\n",
        "\n",
        "    ### Count correctly predicted labels###\n",
        "    correctly_predicted_count = correctly_predicted(output,target)\n",
        "    total_accurate += correctly_predicted_count\n",
        "\n",
        "    ### backward propagation\n",
        "    GD.zero_grad()\n",
        "    batch_loss.backward()\n",
        "    GD.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_loss = total_loss/num_batches\n",
        "  accuracy = total_accurate/num_samples\n",
        "  print(f\"Average loss: {train_loss:4f}, accuracy: {accuracy:.2%}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dLu2EYCoVdFX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 10\n",
        "for i in range(epochs):\n",
        "  print(f\"Traning Start: EPOCH number: {i+1}\")\n",
        "  train(train_loader, model, Loss, GD, i+1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAZwUG-MVo3u",
        "outputId": "b3172528-c72a-4f01-d108-70167789e5d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning Start: EPOCH number: 1\n",
            "\n",
            " >>> Epoch 1, Batch 0\n",
            "\n",
            " >>> Epoch 1, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 1, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 1, batch 100\n",
            "\n",
            " >>> Epoch 1, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 1, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 1, batch 200\n",
            "\n",
            " >>> Epoch 1, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 1, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 1, batch 300\n",
            "\n",
            " >>> Epoch 1, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 1, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 1, batch 400\n",
            "\n",
            " >>> Epoch 1, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 1, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 1, batch 500\n",
            "Average loss: 1.318138, accuracy: 64.73%\n",
            "Traning Start: EPOCH number: 2\n",
            "\n",
            " >>> Epoch 2, Batch 0\n",
            "\n",
            " >>> Epoch 2, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 2, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 2, batch 100\n",
            "\n",
            " >>> Epoch 2, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 2, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 2, batch 200\n",
            "\n",
            " >>> Epoch 2, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 2, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 2, batch 300\n",
            "\n",
            " >>> Epoch 2, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 2, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 2, batch 400\n",
            "\n",
            " >>> Epoch 2, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 2, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 2, batch 500\n",
            "Average loss: 0.431830, accuracy: 89.37%\n",
            "Traning Start: EPOCH number: 3\n",
            "\n",
            " >>> Epoch 3, Batch 0\n",
            "\n",
            " >>> Epoch 3, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 3, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 3, batch 100\n",
            "\n",
            " >>> Epoch 3, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 3, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 3, batch 200\n",
            "\n",
            " >>> Epoch 3, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 3, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 3, batch 300\n",
            "\n",
            " >>> Epoch 3, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 3, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 3, batch 400\n",
            "\n",
            " >>> Epoch 3, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 3, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 3, batch 500\n",
            "Average loss: 0.308745, accuracy: 91.30%\n",
            "Traning Start: EPOCH number: 4\n",
            "\n",
            " >>> Epoch 4, Batch 0\n",
            "\n",
            " >>> Epoch 4, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 4, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 4, batch 100\n",
            "\n",
            " >>> Epoch 4, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 4, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 4, batch 200\n",
            "\n",
            " >>> Epoch 4, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 4, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 4, batch 300\n",
            "\n",
            " >>> Epoch 4, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 4, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 4, batch 400\n",
            "\n",
            " >>> Epoch 4, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 4, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 4, batch 500\n",
            "Average loss: 0.269867, accuracy: 92.14%\n",
            "Traning Start: EPOCH number: 5\n",
            "\n",
            " >>> Epoch 5, Batch 0\n",
            "\n",
            " >>> Epoch 5, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 5, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 5, batch 100\n",
            "\n",
            " >>> Epoch 5, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 5, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 5, batch 200\n",
            "\n",
            " >>> Epoch 5, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 5, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 5, batch 300\n",
            "\n",
            " >>> Epoch 5, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 5, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 5, batch 400\n",
            "\n",
            " >>> Epoch 5, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 5, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 5, batch 500\n",
            "Average loss: 0.244174, accuracy: 92.77%\n",
            "Traning Start: EPOCH number: 6\n",
            "\n",
            " >>> Epoch 6, Batch 0\n",
            "\n",
            " >>> Epoch 6, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 6, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 6, batch 100\n",
            "\n",
            " >>> Epoch 6, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 6, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 6, batch 200\n",
            "\n",
            " >>> Epoch 6, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 6, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 6, batch 300\n",
            "\n",
            " >>> Epoch 6, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 6, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 6, batch 400\n",
            "\n",
            " >>> Epoch 6, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 6, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 6, batch 500\n",
            "Average loss: 0.229601, accuracy: 93.14%\n",
            "Traning Start: EPOCH number: 7\n",
            "\n",
            " >>> Epoch 7, Batch 0\n",
            "\n",
            " >>> Epoch 7, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 7, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 7, batch 100\n",
            "\n",
            " >>> Epoch 7, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 7, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 7, batch 200\n",
            "\n",
            " >>> Epoch 7, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 7, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 7, batch 300\n",
            "\n",
            " >>> Epoch 7, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 7, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 7, batch 400\n",
            "\n",
            " >>> Epoch 7, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 7, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 7, batch 500\n",
            "Average loss: 0.211819, accuracy: 93.69%\n",
            "Traning Start: EPOCH number: 8\n",
            "\n",
            " >>> Epoch 8, Batch 0\n",
            "\n",
            " >>> Epoch 8, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 8, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 8, batch 100\n",
            "\n",
            " >>> Epoch 8, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 8, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 8, batch 200\n",
            "\n",
            " >>> Epoch 8, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 8, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 8, batch 300\n",
            "\n",
            " >>> Epoch 8, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 8, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 8, batch 400\n",
            "\n",
            " >>> Epoch 8, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 8, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 8, batch 500\n",
            "Average loss: 0.209527, accuracy: 93.67%\n",
            "Traning Start: EPOCH number: 9\n",
            "\n",
            " >>> Epoch 9, Batch 0\n",
            "\n",
            " >>> Epoch 9, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 9, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 9, batch 100\n",
            "\n",
            " >>> Epoch 9, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 9, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 9, batch 200\n",
            "\n",
            " >>> Epoch 9, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 9, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 9, batch 300\n",
            "\n",
            " >>> Epoch 9, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 9, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 9, batch 400\n",
            "\n",
            " >>> Epoch 9, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 9, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 9, batch 500\n",
            "Average loss: 0.198420, accuracy: 93.92%\n",
            "Traning Start: EPOCH number: 10\n",
            "\n",
            " >>> Epoch 10, Batch 0\n",
            "\n",
            " >>> Epoch 10, Batch 100\n",
            "\n",
            " Saving checkpoint for epoch 10, batch 100\n",
            "Checkpoint saved and downloaded on local for epoch 10, batch 100\n",
            "\n",
            " >>> Epoch 10, Batch 200\n",
            "\n",
            " Saving checkpoint for epoch 10, batch 200\n",
            "Checkpoint saved and downloaded on local for epoch 10, batch 200\n",
            "\n",
            " >>> Epoch 10, Batch 300\n",
            "\n",
            " Saving checkpoint for epoch 10, batch 300\n",
            "Checkpoint saved and downloaded on local for epoch 10, batch 300\n",
            "\n",
            " >>> Epoch 10, Batch 400\n",
            "\n",
            " Saving checkpoint for epoch 10, batch 400\n",
            "Checkpoint saved and downloaded on local for epoch 10, batch 400\n",
            "\n",
            " >>> Epoch 10, Batch 500\n",
            "\n",
            " Saving checkpoint for epoch 10, batch 500\n",
            "Checkpoint saved and downloaded on local for epoch 10, batch 500\n",
            "Average loss: 0.191320, accuracy: 94.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing #####\n",
        "\n",
        "def test(data_loader,model,Loss,GD):\n",
        "\n",
        "  #model.eval()\n",
        "\n",
        "  num_batches_test = len(data_loader)\n",
        "  num_samples_test = len(data_loader.dataset)\n",
        "\n",
        "  total_loss_test = 0\n",
        "  total_accurate_test = 0\n",
        "\n",
        "\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(data_loader):\n",
        "\n",
        "    print(f\"Batch ID = {batch_idx}\")\n",
        "    ## copy to device\n",
        "    data = data.view(-1, 28 * 28).to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    ### forward pass\n",
        "    output = model(data)\n",
        "    #print(f\"Length of output = {len(output)}\")\n",
        "\n",
        "    ### Calculate loss ###\n",
        "    batch_loss = Loss(output,target)\n",
        "    total_loss_test += batch_loss\n",
        "\n",
        "    ### Count correctly predicted labels###\n",
        "    correctly_predicted_count = correctly_predicted(output,target)\n",
        "    total_accurate_test += correctly_predicted_count\n",
        "\n",
        "\n",
        "  test_loss = total_loss_test/num_batches_test\n",
        "  accuracy_test = total_accurate_test/num_samples_test\n",
        "  print(f\"Average loss: {test_loss:4f}, accuracy: {accuracy_test:.2%}\")\n",
        "\n",
        "\n",
        "test(test_loader,model,Loss,GD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "nBL0yf8QJ9uA",
        "outputId": "afbdd934-32d7-43df-d6c5-9e5b12df4f42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f8305802dbde>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ]
    }
  ]
}